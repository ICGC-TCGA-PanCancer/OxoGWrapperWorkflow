package com.github.seqware;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.nio.file.StandardOpenOption;
import java.util.ArrayList;
import java.util.List;

import net.sourceforge.seqware.pipeline.workflowV2.AbstractWorkflowDataModel;
import net.sourceforge.seqware.pipeline.workflowV2.model.Job;

public class OxoGWrapperWorkflow extends AbstractWorkflowDataModel {

	private String oxoQScore = "";
	// private String donorID;
	private String aliquotID;
	private String bamNormalObjectID;
	private String bamTumourObjectID;
	private String sangerVCFObjectID;
	private String dkfzemblVCFObjectID;
	private String broadVCFObjectID;
	private String museVCFObjectID;
	
	private String uploadURL;

	private String JSONrepo = null;
	private String JSONrepoName = "oxog-ops";
	private String JSONfolderName = null;
	private String JSONlocation = "/datastore/gitroot";
	private String JSONfileName = null;

	private String GITemail = "";
	private String GITname = "ICGC AUTOMATION";
	private String GITPemFile = "";

	//These will be needed so that vcf-uploader can generate the analysis.xml and manifest.xml files
	private String tumourMetdataURL;
	private String normalMetdataURL;
	
	private String tumourBAM;
	private String normalBAM;
	private String sangerVCF;
	private String dkfzEmblVCF;
	private String broadVCF;
	private String museVCF;
	
	private int snvPadding = 10;
	private int svPadding = 10;
	private int indelPadding = 200;
	
	private String storageSource = "collab";
	
	private boolean gitMoveTestMode = false;
	
	/* # of files to upload:
	 * 1 OxoG SNV
	 * 	1 index file
	 * 2 minibams
	 * 	2 index files
	 * 3 normalized indels
	 * 	3 index files
	 * 1 OxoG SNV from indels
	 * 	1 index
	 * 3 SV files
	 * 	3 index files
	 * ... and then some from Jonathan's annotation process... (TBD)
	 */
	private List<String> filesToUpload = new ArrayList<String>(25);
	
	//Paths to VCFs generated by merging types across workflows. 
	private String snvVCF;
	private String svVCF;
	private String indelVCF;

	/**
	 * Get a property name that is mandatory
	 * @param propName The name of the property
	 * @return The property, as a String. Convert to other types if you need to.
	 * @throws Exception An Exception with the message "Property with key <i>propName</i> cannot be null" will be thrown if property is not found.
	 */
	private String getMandatoryProperty(String propName) throws Exception
	{
		if (hasPropertyAndNotNull(propName)) {
			return getProperty(propName);
		}
		else {
			throw new Exception ("Property with key "+propName+ " cannot be null!");
		}
	}

	/**
	 * Initial setup.
	 */
	private void init() {
		try {
			
			this.oxoQScore = this.getMandatoryProperty(JSONUtils.OXOQ_SCORE);
			this.JSONrepo = this.getMandatoryProperty("JSONrepo");
			this.JSONrepoName = this.getMandatoryProperty("JSONrepoName");
			this.JSONfolderName = this.getMandatoryProperty("JSONfolderName");
			this.JSONfileName = this.getMandatoryProperty("JSONfileName");
			this.GITemail = this.getMandatoryProperty("GITemail");
			this.GITname = this.getMandatoryProperty("GITname");
			
			this.bamNormalObjectID = this.getMandatoryProperty(JSONUtils.BAM_NORMAL_OBJECT_ID);
			this.bamTumourObjectID = this.getMandatoryProperty(JSONUtils.BAM_TUMOUR_OBJECT_ID);
			this.sangerVCFObjectID = this.getMandatoryProperty(JSONUtils.SANGER_VCF_OBJECT_ID);
			this.dkfzemblVCFObjectID = this.getMandatoryProperty(JSONUtils.DKFZEMBL_VCF_OBJECT_ID);
			this.broadVCFObjectID = this.getMandatoryProperty(JSONUtils.BROAD_VCF_OBJECT_ID);
			this.uploadURL = this.getMandatoryProperty("uploadURL");
			this.aliquotID = this.getMandatoryProperty(JSONUtils.ALIQUOT_ID);
			
			this.GITPemFile = this.getMandatoryProperty("GITPemFile");

			if (hasPropertyAndNotNull("gitMoveTestMode")) {
				//gitMoveTestMode is not mandatory - it should default to false.
				this.gitMoveTestMode = Boolean.valueOf(getProperty("gitMoveTestMode"));
			}
			
			if (hasPropertyAndNotNull("storageSource")) {
				//storageSource is not mandatory - it should default to "collab"
				this.storageSource = getProperty("storageSource");
			}
			
			if (hasPropertyAndNotNull("snvPadding")) {
				//snv padding is not mandatory
				this.snvPadding = Integer.valueOf(getProperty("snvPadding"));
			}
			
			if (hasPropertyAndNotNull("svPadding")) {
				//sv padding is not mandatory
				this.svPadding = Integer.valueOf(getProperty("svPadding"));
			}
			
			if (hasPropertyAndNotNull("indelPadding")) {
				//indel padding is not mandatory
				this.indelPadding = Integer.valueOf(getProperty("indelPadding"));
			}
			
			this.generateRulesFile();
			
		} catch (Exception e) {
			throw new RuntimeException("Exception encountered during workflow init: "+e.getMessage(),e);
		}
	}

	/**
	 * Generates a rules file that is used for the variant program that produces minibams.
	 * @throws URISyntaxException
	 * @throws IOException
	 */
	private void generateRulesFile() throws URISyntaxException, IOException
	{
		Path pathToPaddingRules = Paths.get(new URI("/datastore/padding_rules.txt"));
		String paddingFileString = "pad["+this.svPadding+"];mlregion@/sv.vcf\n"+
									"pad["+this.snvPadding+"];mlregion@/snv.vcf\n"+
									"pad["+this.indelPadding+"];mlregion@/indel.vcf\n";
		
		Files.write(pathToPaddingRules, paddingFileString.getBytes(), StandardOpenOption.CREATE);
	}
	
	/**
	 * Copy the credentials files from ~/.gnos to /datastore/credentials
	 * @param parentJob
	 * @return
	 */
	private Job copyCredentials(Job parentJob){
		Job copy = this.getWorkflow().createBashJob("copy ~/.gnos");
		copy.setCommand("sudo cp -r ~/.gnos /datastore/credentials && ls -l /datastore/credentials");
		copy.addParent(parentJob);
		return copy;
	}
	
	/**
	 * Defines what BAM types there are:
	 * <ul><li>normal</li><li>tumour</li></ul>
	 * @author sshorser
	 *
	 */
	enum BAMType{
		normal,tumour
	}
	/**
	 * Download a BAM file.
	 * @param parentJob
	 * @param objectID - the object ID of the BAM file
	 * @param bamType - is it normal or tumour? This used to determine the name of the directory that the file ends up in.
	 * @return
	 */
	private Job getBAM(Job parentJob, String objectID, BAMType bamType) {
		Job getBamFileJob = this.getWorkflow().createBashJob("get "+bamType.toString()+" BAM file");
		getBamFileJob.addParent(parentJob);
		String storageClientDockerCmdNormal ="sudo docker run --rm"
				+ " -e STORAGE_PROFILE="+this.storageSource+" " 
			    + " -v /datastore/bam/"+bamType.toString()+"/logs/:/icgc/icgc-storage-client/logs/:rw "
				+ " -v /datastore/credentials/collab.token:/icgc/icgc-storage-client/conf/application.properties:ro "
			    + " -v /datastore/bam/"+bamType.toString()+"/:/tmp/:rw"
			    + " icgc/icgc-storage-client "
				+ " /icgc/icgc-storage-client/bin/icgc-storage-client download --object-id "
					+ objectID +" --output-dir /tmp/";
		getBamFileJob.setCommand(storageClientDockerCmdNormal);

		return getBamFileJob;
	}

	/**
	 * Defines the different pipelines:
	 * <ul>
	 * <li>sanger</li>
	 * <li>dkfz_embl</li>
	 * <li>broad</li>
	 * <li>muse</li>
	 * </ul>
	 * @author sshorser
	 *
	 */
	enum Pipeline {
		sanger, dkfz_embl, broad, muse
	}
	/**
	 This will download VCFs for a workflow, based on an object ID(s).
	 It will perform these operations:
	 <ol>
	 <li>download VCFs</li>
	 <li>normalize INDEL VCF</li>
	 <li>extract SNVs from INDEL into a separate VCF</li>
	 </ol>
	 * @param parentJob
	 * @param workflowName The pipeline (AKA workflow) that the VCFs come from. This will determine the name of the output directory where the downloaded files will be stored.
	 * @param objectID
	 * @return
	 */
	private Job getVCF(Job parentJob, Pipeline workflowName, String objectID) {
		Job getVCFJob = this.getWorkflow().createBashJob("get VCF for workflow " + workflowName);
		String outDir = "/datastore/vcf/"+workflowName;
		//TODO: Will need multiple downloads if the object-id will only download one but we need three (SV, SNV, INDEL)
		String getVCFCommand = "sudo docker run --rm"
				+ " -e STORAGE_PROFILE="+this.storageSource+" " 
			    + " -v "+outDir+"/logs/:/icgc/icgc-storage-client/logs/:rw "
				+ " -v /datastore/credentials/collab.token:/icgc/icgc-storage-client/conf/application.properties:ro "
			    + " -v "+outDir+"/:/tmp/:rw"
	    		+ " icgc/icgc-storage-client "
				+ " /icgc/icgc-storage-client/bin/icgc-storage-client download --object-id " + objectID+" --output-dir /tmp/";
		getVCFJob.setCommand(getVCFCommand);
		getVCFJob.addParent(parentJob);



		return getVCFJob;
	}

	private Job preProcessVCF(Job parent, Pipeline workflowName )
	{
		String outDir = "/datastore/vcf/"+workflowName;
		// TODO: Many of these steps below could probably be combined into a single Job
		// that makes runs a single docker container, but executes multiple commands.
		Job bcfToolsNormJob = this.getWorkflow().createBashJob("run VCF primitives on indel");
		String runBCFToolsNormCommand = "sudo docker run --rm "
					+ " -v "+outDir+"/*.somatic.indel.vcf.gz:/datastore/datafile.vcf.gz "
					+ " -v /datastore/refdata/public:/ref"
					+ " compbio/ngseasy-base:a1.0-002 " 
					+ " bcftools norm -c w -m -any -O -z -f /ref/Homo_sapiens_assembly19.fasta  /datastore/datafile.vcf.gz "  
				+ " > "+outDir+"/somatic.indel.bcftools-norm.vcf.gz "
				+ " && tabix -f -p vcf "+outDir+"/somatic.indel.bcftools-norm.vcf.gz ";
		bcfToolsNormJob.setCommand(runBCFToolsNormCommand);
		bcfToolsNormJob.addParent(parent);
		
		//Normalized INDELs should be indexed uploaded
		filesToUpload.add(outDir+"/somatic.indel.bcftools-norm.vcf.gz");
		filesToUpload.add(outDir+"/somatic.indel.bcftools-norm.vcf.gz.tbi");
		
		Job extractSNVFromIndel = this.getWorkflow().createBashJob("extracting SNVs from INDEL");
		extractSNVFromIndel.setCommand("bgzip -d -c "+outDir+"/somatic.indel.bcftools-norm.vcf.gz > "+outDir+"/somatic.indel.bcftools-norm.vcf && grep -e '^#' -i -e '^[^#].*[[:space:]][ACTG][[:space:]][ACTG][[:space:]]' "+outDir+"/somatic.indel.bcftools-norm.vcf "
										+ "> "+outDir+"/somatic.indel.bcftools-norm.extracted-snvs.vcf "
												+ " && bgzip "+outDir+"/somatic.indel.bcftools-norm.extracted-snvs.vcf "
												+ " && tabix -f -p vcf "+outDir+"/somatic.indel.bcftools-norm.extracted-snvs.vcf ");
		extractSNVFromIndel.addParent(bcfToolsNormJob);
		
		if (workflowName.equals("Sanger"))
			this.sangerVCF = outDir + "/snv_AND_indel_AND_sv.vcf.gz";
		else if (workflowName.equals("DKFZ_EMBL"))
			this.dkfzEmblVCF = outDir + "/snv_AND_indel_AND_sv.vcf.gz";
		else if (workflowName.equals("Broad"))
			this.broadVCF = outDir + "/snv_AND_indel_AND_sv.vcf.gz";
	
		return extractSNVFromIndel;
	}
	
	/**
	 * The types of VCF files there are:
	 * <ul>
	 * <li>sv</li>
	 * <li>snv</li>
	 * <li>indel</li>
	 * </ul>
	 * @author sshorser
	 *
	 */
	enum VCFType{
		sv, snv, indel
	}
	/**
	 * This will combine VCFs from different workflows by the same type. All INDELs will be combined into a new output file,
	 * all SVs will be combined into a new file, all SNVs will be combined into a new file. 
	 * @param parents
	 * @return
	 */
	private Job combineVCFsByType(Job ... parents)
	{
		Job vcfCombineJob = this.getWorkflow().createBashJob("Combining VCFs by type");
		
		//run the merge script, then bgzip and index them all.
		vcfCombineJob.setCommand("perl "+this.getWorkflowBaseDir()+"/scripts/vcf_merge_by_type.pl "
				+ " broad_snv.vcf sanger_snv.vcf de_snv.vcf "
				+ " broad_indel.vcf sanger_indel.vcf de_indel.vcf"
				+ " broad_sv.vcf sanger_sv.vcf de_sv.vcf /datastore/vcf/ /datastore/merged_vcfs/ ; \n"
				+ " bgzip -c /datastore/merged_vcfs/snv.clean.sorted.vcf > /datastore/merged_vcfs/snv.clean.sorted.vcf.gz ; \n"
				+ " bgzip -c /datastore/merged_vcfs/sv.clean.sorted.vcf > /datastore/merged_vcfs/sv.clean.sorted.vcf.gz ; \n"
				+ " bgzip -c /datastore/merged_vcfs/indel.clean.sorted.vcf > /datastore/merged_vcfs/indel.clean.sorted.vcf.gz ; \n"
				+ " tabix -f /datastore/merged_vcfs/snv.clean.sorted.vcf.gz ; \n"
				+ " tabix -f /datastore/merged_vcfs/sv.clean.sorted.vcf.gz ; \n"
				+ " tabix -f /datastore/merged_vcfs/indel.clean.sorted.vcf.gz ; \n");
		
		for (Job parent : parents)
		{
			vcfCombineJob.addParent(parent);
		}
		
		this.snvVCF = "/datastore/merged_vcfs/snv.clean.sorted.vcf";
		this.svVCF = "/datastore/merged_vcfs/sv.clean.sorted.vcf";
		this.indelVCF = "/datastore/merged_vcfs/indel.clean.sorted.vcf";
		
		return vcfCombineJob;
	}
	
	/**
	 * Runs the OxoG filtering program inside the Broad's OxoG docker container. Output file(s) will be in /datastore/oxog_results/ and the working files will 
	 * be in /datastore/oxog_workspace
	 * @param parent
	 * @return
	 */
	private Job doOxoG(Job parent) {
		Job runOxoGWorkflow = this.getWorkflow().createBashJob("Run OxoG Filter");
		String oxogMounts = " -v /datastore/refdata/:/cga/fh/pcawg_pipeline/refdata/ "
				+ " -v /datastore/oncotator_db/:/cga/fh/pcawg_pipeline/refdata/public/oncotator_db/ "  
				+ " -v /datastore/oxog_workspace/:/cga/fh/pcawg_pipeline/jobResults_pipette/jobs/"+this.aliquotID+"/:rw " 
				+ " -v /datastore/bam/:/datafiles/BAM/  -v /datastore/vcf/:/datafiles/VCF/ "
				+ " -v /datastore/oxog_results/:/cga/fh/pcawg_pipeline/jobResults_pipette/results:rw ";
		String oxogCommand = "/cga/fh/pcawg_pipeline/pipelines/run_one_pipeline.bash pcawg /cga/fh/pcawg_pipeline/pipelines/oxog_pipeline.py "
				+ this.aliquotID + " " + this.tumourBAM + " " + this.normalBAM + " " + this.oxoQScore + " "
				+ this.sangerVCF + " " + this.dkfzEmblVCF + " " + this.broadVCF;
		runOxoGWorkflow.setCommand(
				"sudo docker run --name=\"oxog_filter\" "+oxogMounts+" oxog " + oxogCommand);
		
		runOxoGWorkflow.addParent(parent);
		//Job getLogs = this.getOxoGLogs(runOxoGWorkflow);
		
		//TODO: will probably need to find a way to extract *just* the VCF (and the index - or create a new index) from this tar.
		this.filesToUpload.add("/datastore/oxog_results/*.gnos_files.tar");

		return runOxoGWorkflow;
	}

	/**
	 * This will run the OxoG Filter program on the SNVs that were extracted from the INDELs, if there were any. It's possible that no SNVs will be extracted from any
	 * INDEL files (in fact, I've been told this is the most likely scenario for most donors) in which case nothing will run. See the script scripts/run_oxog_extracted_SNVs.sh
	 * for more details on this.
	 * @param parent
	 * @return
	 */
	private Job doOxoGSnvsFromIndels(Job parent) {
		Job oxoGOnSnvsFromIndels = this.getWorkflow().createBashJob("Running OxoG on SNVs from INDELs");
		String vcfBaseDir = "/datastore/vcf/";
		String vcf1 = vcfBaseDir+Pipeline.broad.toString()+"/somatic.indel.bcftools-norm.extracted-snvs.vcf ";
		String vcf2 = vcfBaseDir+Pipeline.sanger.toString()+"/somatic.indel.bcftools-norm.extracted-snvs.vcf ";
		String vcf3 = vcfBaseDir+Pipeline.dkfz_embl.toString()+"/somatic.indel.bcftools-norm.extracted-snvs.vcf ";
		String vcf4 = vcfBaseDir+Pipeline.muse.toString()+"/somatic.indel.bcftools-norm.extracted-snvs.vcf ";
		oxoGOnSnvsFromIndels.setCommand(this.getWorkflowBaseDir()+"/scripts/run_oxog_extracted_SNVs.sh "+
																vcf1+" "+vcf2+" "+vcf3+" "+vcf4+" "+
																this.normalBAM+" "+this.tumourBAM+" "+
																this.aliquotID+" "+
																this.oxoQScore);
		oxoGOnSnvsFromIndels.addParent(parent);
		//TODO: will probably need to find a way to extract *just* the VCF from this tar.
		//Also, this one will be tricky: the file might not exist, but we can't determine that at 
		//workflow-build time - it will only be known once the scripts start running. Might need 
		//to add this to the upload programatically...
		this.filesToUpload.add("/datastore/oxog_results_extracted_snvs/*.gnos_files.tar");
		
		return oxoGOnSnvsFromIndels;
	}
	
	/**
	 * Runs the variant program inside the Broad's OxoG container to produce a mini-BAM for a given BAM. 
	 * @param parent
	 * @param bamType - The type of BAM file to use. Determines the name of the output file.
	 * @param bamPath - The path to the input BAM file.
	 * @return
	 */
	private Job doVariantBam(Job parent, BAMType bamType, String bamPath) {
		Job runOxoGWorkflow = this.getWorkflow().createBashJob("Run variantbam");
//		String oxogMounts = " -v "+bamPath+":/input.bam "
//				+ " -v "+this.snvVCF+":/snv.vcf "
//				+ " -v "+this.svVCF+":/sv.vcf "
//				+ " -v "+this.indelVCF+":/indel.vcf "
//				+ " -v /datastore/padding_rules.txt:/rules.txt "
//				+ " -v /datastore/variantbam_results/:/outdir/:rw ";
//		// TODO: Update this to use the per-VCF-type combined VCFs instead of the per-workflow combined VCFs.
//		String oxogCommand = " /bin/bash -c \" /cga/fh/pcawg_pipeline/modules/VariantBam/variant "
//				+ " -o /outdir/minibam_"+bamType+".bam"
//				+ " -i /input.bam "
//				+ " -r /rules.txt "
//				+ " -l /snv.vcf "
//				+ " -l /sv.vcf "
//				+ " -l /indel.vcf \" ";
//				
//		runOxoGWorkflow.setCommand(
//				"sudo docker run --name=\"variantbam_"+bamType+"\" "+oxogMounts+" oxog " + oxogCommand);

		String command = DockerCommandCreator.createVariantBamCommand(bamType, bamPath, this.snvVCF, this.svVCF, this.indelVCF);
		runOxoGWorkflow.setCommand(command);
		//The bam file will need to be indexed!
		runOxoGWorkflow.getCommand().addArgument("\nsamtools index /datastore/variantbam_results/minibam_"+bamType+".bam");
		
		this.filesToUpload.add("/datastore/variantbam_results/minibam_"+bamType+".bam");
		this.filesToUpload.add("/datastore/variantbam_results/minibam_"+bamType+".bai");
		runOxoGWorkflow.addParent(parent);
		
		//Job getLogs = this.getOxoGLogs(runOxoGWorkflow);

		return runOxoGWorkflow;
	}
	
	/**
	 * Gets logs from the container named oxog_run
	 * @param parent
	 * @return
	 * 
	 */
	@Deprecated
	private Job getOxoGLogs(Job parent) {
		//TODO: Either update this to make it more relevant or remove it.
		Job getLog = this.getWorkflow().createBashJob("get OxoG docker logs");
		// This will get the docker logs and print them to stdout, but we may also want to get the logs
		// in the mounted oxog_workspace dir...
		getLog.setCommand("sudo docker logs oxog_run");
		getLog.addParent(parent);
		return getLog;
	}

	/**
	 * Uploads files... TBC...
	 * @param parentJob
	 * @return
	 */
	private Job doUpload(Job parentJob) {
		// Will need to run gtupload to generate the analysis.xml and manifest files (but not actually upload). 
		// The tar file contains all results.
		Job generateAnalysisFiles = this.getWorkflow().createBashJob("generate_analysis_files_for_upload");
		
		//Files to upload:
		//OxoG files
		//minibams
		//other intermediate files?
		
		//first thing to do is generate MD5 sums for all uploadable files.
		for (String file : this.filesToUpload)
		{
			//md5sum test_files/tumour_minibam.bam.bai | cut -d ' ' -f 1 > test_files/tumour_minibam.bai.md5
			generateAnalysisFiles.getCommand().addArgument(" md5sum "+file+" | cut -d ' ' -f 1 > "+file+".md5 \n");
		}
		
		
		//Note: It was decided there should be two uploads: one for minibams and one for VCFs (for people who want VCFs but not minibams).
		
		Job uploadResults = this.getWorkflow().createBashJob("upload results");
		uploadResults.setCommand("rsync /cga/fh/pcawg_pipeline/jobResults_pipette/results/" + this.aliquotID
				+ ".oxoG.somatic.snv_mnv.vcf.gz.tar  " + this.uploadURL);
		uploadResults.addParent(parentJob);
		return uploadResults;
	}
	


	/**
	 * Build the workflow!!
	 */
	@Override
	public void buildWorkflow() {
		try {
			this.init();
			// Pull the repo.
			Job pullRepo = GitUtils.pullRepo(this.getWorkflow(), this.GITPemFile, this.GITname, this.JSONrepo, this.JSONrepoName, this.JSONlocation, this.GITemail  );
			
			Job copy = this.copyCredentials(pullRepo);
			
			// indicate job is in downloading stage.
			Job move2download = GitUtils.gitMove("queued-jobs", "downloading-jobs", this.getWorkflow(), this.JSONlocation, this.JSONrepoName, this.JSONfolderName, this.GITname, this.GITemail, this.gitMoveTestMode, this.JSONfileName ,copy);
			// These jobs will all reun parallel. The BAM jobs just download, but the VCF jobs also do some
			// processing (bcftools norm and vcfcombine) on the downloaded files.
			Job downloadNormalBam = this.getBAM(move2download,this.bamNormalObjectID,BAMType.normal);
			this.normalBAM = "/datastore/bam/normal/*.bam";
			Job downloadTumourBam = this.getBAM(move2download,this.bamTumourObjectID,BAMType.tumour);
			this.tumourBAM = "/datastore/bam/tumour/*.bam";
			
			//Download jobs
			Job downloadSangerVCFs = this.getVCF(move2download, Pipeline.sanger, this.sangerVCFObjectID);
			Job downloadDkfzEmblVCFs = this.getVCF(move2download, Pipeline.dkfz_embl, this.dkfzemblVCFObjectID);
			Job downloadBroadVCFs = this.getVCF(move2download, Pipeline.broad, this.broadVCFObjectID);
			
			// After we've downloaded all VCFs on a per-workflow basis, we also need to do a vcfcombine 
			// on the *types* of VCFs, for the minibam generator. The per-workflow combined VCFs will
			// be used by the OxoG filter. These three can be done in parallel because they all require the same inputs, 
			// but none require the inputs of the other and they are not very intense jobs.
			
			// indicate job is running.
			Job move2running = GitUtils.gitMove( "queued-jobs", "running-jobs", this.getWorkflow(),
					this.JSONlocation, this.JSONrepoName, this.JSONfolderName, this.GITname, this.GITemail, this.gitMoveTestMode, this.JSONfileName
					, downloadSangerVCFs, downloadDkfzEmblVCFs, downloadBroadVCFs, downloadNormalBam, downloadTumourBam);

			// OxoG will run after move2running. Move2running will run after all the jobs that perform input file downloads and file preprocessing have finished.  
			Job sangerPreprocessVCF = this.preProcessVCF(move2running, Pipeline.sanger);
			Job dkfzEmblPreprocessVCF = this.preProcessVCF(move2running, Pipeline.dkfz_embl);
			Job broadPreprocessVCF = this.preProcessVCF(move2running, Pipeline.broad);
			
			Job combineVCFsByType = this.combineVCFsByType( sangerPreprocessVCF, dkfzEmblPreprocessVCF, broadPreprocessVCF);
			
			Job oxoG = this.doOxoG(combineVCFsByType);
			Job oxoGSnvsFromIndels = this.doOxoGSnvsFromIndels(oxoG);
			// variantbam jobs will run parallel to oxog. variant seems to only use a *single* core, but runs long ( 60 - 120 min on OpenStack);
			// OxoG uses a few cores (sometimes), but runs shorter (~20 min, on OpenStack).
			Job normalVariantBam = this.doVariantBam(move2running,BAMType.normal,this.normalBAM);
			Job tumourVariantBam = this.doVariantBam(move2running,BAMType.tumour,this.tumourBAM);
	
			// indicate job is in uploading stage.
			Job move2uploading = GitUtils.gitMove("running-jobs", "uploading-jobs", this.getWorkflow(), this.JSONlocation, this.JSONrepoName, this.JSONfolderName, this.GITname, this.GITemail, this.gitMoveTestMode, this.JSONfileName , oxoGSnvsFromIndels, normalVariantBam, tumourVariantBam);
			
			//Now do the Upload
			Job uploadResults = doUpload(move2uploading);
	
			// indicate job is complete.
			/*Job move2finished = */GitUtils.gitMove( "uploading-jobs", "completed-jobs", this.getWorkflow(), this.JSONlocation, this.JSONrepoName, this.JSONfolderName, this.GITname, this.GITemail, this.gitMoveTestMode, this.JSONfileName , uploadResults);
		}
		catch (Exception e)
		{
			throw new RuntimeException ("Exception caught: "+e.getMessage(), e);
		}
	}
}
